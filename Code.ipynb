{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project in Learning Based Inertial Sensing\n",
    "\n",
    "This is a final project in the course \"Learning Based Inertial Sensing\" at the Technion, Israel.\n",
    "This project aims to classify the road surface type which a car is driving upon.\n",
    "\n",
    "The project is based on an existing research: \n",
    "J. Menegazzo and A. von Wangenheim, \"Multi-Contextual and Multi-Aspect Analysis for Road Surface Type Classification Through Inertial Sensors and Deep Learning,\" 2020 X Brazilian Symposium on Computing Systems Engineering (SBESC), Florianopolis, 2020, pp. 1-8, doi: 10.1109/SBESC51047.2020.9277846.\n",
    "\n",
    "We will explore various machine learning tricks to try to better our understanding and intuition on neural networks, and hopefully outperform the existing research on the matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
    "---\n",
    "\n",
    "\n",
    "|Name     |Campus Email| ID  |\n",
    "|---------|--------------------------------|----------|\n",
    "|Eyal Kaldor| eyalkaldor@campus.technion.ac.il| 205907330|\n",
    "|Snir Carmeli| snircarmeli@campus.technion.ac.il| 318880234|\n",
    "|Dolev Freund| dolev@campus.technion.ac.il| 316216605|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Add Headline -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy, pytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure we are using the GPU for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Work on GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "col_names = ['timestamp', 'acc_x_dashboard', 'acc_y_dashboard', 'acc_z_dashboard', 'acc_x_above_suspension', 'acc_y_above_suspension', 'acc_z_above_suspension',\n",
    "              'acc_x_below_suspension', 'acc_y_below_suspension', 'acc_z_below_suspension', 'gyro_x_dashboard', 'gyro_y_dashboard', 'gyro_z_dashboard', \n",
    "              'gyro_x_above_suspension', 'gyro_y_above_suspension', 'gyro_z_above_suspension', 'gyro_x_below_suspension', 'gyro_y_below_suspension',\n",
    "                'gyro_z_below_suspension', 'mag_x_dashboard', 'mag_y_dashboard', 'mag_z_dashboard', 'mag_x_above_suspension', 'mag_y_above_suspension',\n",
    "                'mag_z_above_suspension', 'temp_dashboard', 'temp_above_suspension', 'temp_below_suspension']\n",
    "features = col_names # all columns are features\n",
    "labels_names = ['paved_road', 'unpaved_road', 'dirt_road', 'cobblestone_road', 'asphalt_road', 'no_speed_bump', 'speed_bump_asphalt', 'speed_bump_cobblestone',\n",
    "           'good_road_left', 'regular_road_left', 'bad_road_left', 'good_road_right', 'regular_road_right', 'bad_road_right']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset:\n",
    "Every data points is all the rows of a single time stamp from all 4 files.\n",
    "Between every folder data we will add padding of 1000 rows of zeros to make sure the model will not learn the transition between the folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from folder:  PVS 1 \n",
      "\n",
      "Processed 6.94% of data from folder: PVS 1\n",
      "Processed 13.89% of data from folder: PVS 1\n",
      "Processed 20.83% of data from folder: PVS 1\n",
      "Processed 27.77% of data from folder: PVS 1\n",
      "Processed 34.71% of data from folder: PVS 1\n",
      "Processed 41.66% of data from folder: PVS 1\n",
      "Processed 48.60% of data from folder: PVS 1\n",
      "Processed 55.54% of data from folder: PVS 1\n",
      "Processed 62.48% of data from folder: PVS 1\n",
      "Processed 69.43% of data from folder: PVS 1\n",
      "Processed 76.37% of data from folder: PVS 1\n",
      "Processed 83.31% of data from folder: PVS 1\n",
      "Processed 90.26% of data from folder: PVS 1\n",
      "Processed 97.20% of data from folder: PVS 1\n",
      "\n",
      "Added padding to data from folder:  PVS 1 \n",
      "\n",
      "Loading data from folder:  PVS 2 \n",
      "\n",
      "Processed 8.02% of data from folder: PVS 2\n",
      "Processed 16.04% of data from folder: PVS 2\n",
      "Processed 24.06% of data from folder: PVS 2\n",
      "Processed 32.08% of data from folder: PVS 2\n",
      "Processed 40.10% of data from folder: PVS 2\n",
      "Processed 48.12% of data from folder: PVS 2\n",
      "Processed 56.14% of data from folder: PVS 2\n",
      "Processed 64.16% of data from folder: PVS 2\n",
      "Processed 72.18% of data from folder: PVS 2\n",
      "Processed 80.20% of data from folder: PVS 2\n",
      "Processed 88.22% of data from folder: PVS 2\n",
      "Processed 96.24% of data from folder: PVS 2\n",
      "\n",
      "Added padding to data from folder:  PVS 2 \n",
      "\n",
      "Loading data from folder:  PVS 3 \n",
      "\n",
      "Processed 9.45% of data from folder: PVS 3\n",
      "Processed 18.90% of data from folder: PVS 3\n",
      "Processed 28.35% of data from folder: PVS 3\n",
      "Processed 37.80% of data from folder: PVS 3\n",
      "Processed 47.25% of data from folder: PVS 3\n",
      "Processed 56.70% of data from folder: PVS 3\n",
      "Processed 66.15% of data from folder: PVS 3\n",
      "Processed 75.60% of data from folder: PVS 3\n",
      "Processed 85.05% of data from folder: PVS 3\n",
      "Processed 94.50% of data from folder: PVS 3\n",
      "\n",
      "Added padding to data from folder:  PVS 3 \n",
      "\n",
      "Loading data from folder:  PVS 4 \n",
      "\n",
      "Processed 7.55% of data from folder: PVS 4\n",
      "Processed 15.10% of data from folder: PVS 4\n",
      "Processed 22.64% of data from folder: PVS 4\n",
      "Processed 30.19% of data from folder: PVS 4\n",
      "Processed 37.74% of data from folder: PVS 4\n",
      "Processed 45.29% of data from folder: PVS 4\n",
      "Processed 52.83% of data from folder: PVS 4\n",
      "Processed 60.38% of data from folder: PVS 4\n",
      "Processed 67.93% of data from folder: PVS 4\n",
      "Processed 75.48% of data from folder: PVS 4\n",
      "Processed 83.02% of data from folder: PVS 4\n",
      "Processed 90.57% of data from folder: PVS 4\n",
      "Processed 98.12% of data from folder: PVS 4\n",
      "\n",
      "Added padding to data from folder:  PVS 4 \n",
      "\n",
      "Loading data from folder:  PVS 5 \n",
      "\n",
      "Processed 7.47% of data from folder: PVS 5\n",
      "Processed 14.94% of data from folder: PVS 5\n",
      "Processed 22.41% of data from folder: PVS 5\n",
      "Processed 29.88% of data from folder: PVS 5\n",
      "Processed 37.35% of data from folder: PVS 5\n",
      "Processed 44.82% of data from folder: PVS 5\n",
      "Processed 52.29% of data from folder: PVS 5\n",
      "Processed 59.76% of data from folder: PVS 5\n",
      "Processed 67.23% of data from folder: PVS 5\n",
      "Processed 74.70% of data from folder: PVS 5\n",
      "Processed 82.16% of data from folder: PVS 5\n",
      "Processed 89.63% of data from folder: PVS 5\n",
      "Processed 97.10% of data from folder: PVS 5\n",
      "\n",
      "Added padding to data from folder:  PVS 5 \n",
      "\n",
      "Loading data from folder:  PVS 6 \n",
      "\n",
      "Processed 10.39% of data from folder: PVS 6\n",
      "Processed 20.77% of data from folder: PVS 6\n",
      "Processed 31.16% of data from folder: PVS 6\n",
      "Processed 41.55% of data from folder: PVS 6\n",
      "Processed 51.93% of data from folder: PVS 6\n",
      "Processed 62.32% of data from folder: PVS 6\n",
      "Processed 72.71% of data from folder: PVS 6\n",
      "Processed 83.09% of data from folder: PVS 6\n",
      "Processed 93.48% of data from folder: PVS 6\n",
      "\n",
      "Added padding to data from folder:  PVS 6 \n",
      "\n",
      "Loading data from folder:  PVS 7 \n",
      "\n",
      "Processed 7.78% of data from folder: PVS 7\n",
      "Processed 15.56% of data from folder: PVS 7\n",
      "Processed 23.34% of data from folder: PVS 7\n",
      "Processed 31.12% of data from folder: PVS 7\n",
      "Processed 38.90% of data from folder: PVS 7\n",
      "Processed 46.68% of data from folder: PVS 7\n",
      "Processed 54.45% of data from folder: PVS 7\n",
      "Processed 62.23% of data from folder: PVS 7\n",
      "Processed 70.01% of data from folder: PVS 7\n",
      "Processed 77.79% of data from folder: PVS 7\n",
      "Processed 85.57% of data from folder: PVS 7\n",
      "Processed 93.35% of data from folder: PVS 7\n",
      "\n",
      "Added padding to data from folder:  PVS 7 \n",
      "\n",
      "Loading data from folder:  PVS 8 \n",
      "\n",
      "Processed 8.09% of data from folder: PVS 8\n",
      "Processed 16.18% of data from folder: PVS 8\n",
      "Processed 24.27% of data from folder: PVS 8\n",
      "Processed 32.36% of data from folder: PVS 8\n",
      "Processed 40.45% of data from folder: PVS 8\n",
      "Processed 48.54% of data from folder: PVS 8\n",
      "Processed 56.63% of data from folder: PVS 8\n",
      "Processed 64.72% of data from folder: PVS 8\n",
      "Processed 72.80% of data from folder: PVS 8\n",
      "Processed 80.89% of data from folder: PVS 8\n",
      "Processed 88.98% of data from folder: PVS 8\n",
      "Processed 97.07% of data from folder: PVS 8\n",
      "\n",
      "Added padding to data from folder:  PVS 8 \n",
      "\n",
      "Loading data from folder:  PVS 9 \n",
      "\n",
      "Processed 10.92% of data from folder: PVS 9\n",
      "Processed 21.84% of data from folder: PVS 9\n",
      "Processed 32.77% of data from folder: PVS 9\n",
      "Processed 43.69% of data from folder: PVS 9\n",
      "Processed 54.61% of data from folder: PVS 9\n",
      "Processed 65.53% of data from folder: PVS 9\n",
      "Processed 76.46% of data from folder: PVS 9\n",
      "Processed 87.38% of data from folder: PVS 9\n",
      "Processed 98.30% of data from folder: PVS 9\n",
      "\n",
      "Data shape: 1160905 120\n",
      "Labels shape: 1160905 14\n"
     ]
    }
   ],
   "source": [
    "# There are 9 folders with data: PVS 1-9. Each folder contains these data files: \n",
    "# 1. dataset_gps_mpu_left.csv\n",
    "# 2. dataset_gps_mpu_right.csv\n",
    "# 3. dataset_mpu_left.csv\n",
    "# 4. dataset_mpu_right.csv\n",
    "# The output (labels) are in the file dataset_labels.csv\n",
    "data_files = ['dataset_gps_mpu_left.csv', 'dataset_gps_mpu_right.csv', 'dataset_mpu_left.csv', 'dataset_mpu_right.csv']\n",
    "data_folders = ['PVS 1', 'PVS 2', 'PVS 3', 'PVS 4', 'PVS 5', 'PVS 6', 'PVS 7', 'PVS 8', 'PVS 9']\n",
    "labels_file = 'dataset_labels.csv'\n",
    "# Arrange data in an array: take a window of x samples from all files and stack them together as a row in the array\n",
    "# Print every k rows to see the progress\n",
    "k_print = 10000\n",
    "# Padding for the data array: Each file will be separated by this many rows of zeros\n",
    "padding = 1e4\n",
    "\n",
    "\n",
    "data = []\n",
    "labels_data = []\n",
    "\n",
    "\n",
    "for folder in data_folders:\n",
    "    cnt = 0\n",
    "    files_data = np.hstack([np.genfromtxt(folder + '/' + file, delimiter=',', skip_header=1) for file in data_files])\n",
    "    total_columns = sum([np.genfromtxt(folder + '/' + file, delimiter=',', skip_header=1).shape[1] for file in data_files])\n",
    "    files_labels = np.genfromtxt(folder + '/' + labels_file, delimiter=',', skip_header=1)\n",
    "    print(\"Loading data from folder: \", folder, \"\\n\")\n",
    "\n",
    "    num_samples = files_data.shape[0]\n",
    "    # Preallocate space for data and labels\n",
    "    folder_data = np.zeros((num_samples, files_data.shape[1]))\n",
    "    folder_labels = np.zeros((num_samples, files_labels.shape[1]))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        folder_data[i] = files_data[i, :]\n",
    "        folder_labels[i] = files_labels[i, :]\n",
    "        cnt += 1\n",
    "        if cnt % k_print == 0:\n",
    "            print(\"Processed {:.2f}% of data from folder: {}\".format(100 * cnt / num_samples, folder))\n",
    "\n",
    "    # Append the folder data and labels to the main lists\n",
    "    data.extend(folder_data)\n",
    "    labels_data.extend(folder_labels)\n",
    "        \n",
    "    # Add padding if this is not the last folder\n",
    "    if folder != data_folders[-1]:\n",
    "        zero_rows_data = np.zeros((int(padding), len(data[0])))\n",
    "        zero_rows_labels = np.zeros((int(padding), len(labels_data[0])))\n",
    "        data.extend(zero_rows_data)\n",
    "        labels_data.extend(zero_rows_labels)    \n",
    "        print()\n",
    "        print(\"Added padding to data from folder: \", folder, \"\\n\")\n",
    "\n",
    "# Show shape of data and labels without turning them into torch tensors or numpy arrays\n",
    "print(\"\\nData shape:\", len(data), len(data[0]))\n",
    "print(\"Labels shape:\", len(labels_data), len(labels_data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data shape: torch.Size([1160905, 120])\n",
      "Labels shape: torch.Size([1160905, 14])\n"
     ]
    }
   ],
   "source": [
    "# Convert data and labels to torch tensors\n",
    "data = torch.tensor(data, dtype=torch.float)\n",
    "labels_data = torch.tensor(labels_data, dtype=torch.float)\n",
    "\n",
    "# Show shape of data and labels after turning them into torch\n",
    "print(\"\\nData shape:\", data.shape)\n",
    "print(\"Labels shape:\", labels_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into train, validation and test sets\n",
    "def train_val_test_split(data, labels, train_size=0.7, val_size=0.15, test_size=0.15, random_state=None):\n",
    "    assert train_size + val_size + test_size == 1, \"Train, validation and test sizes must sum to 1\"\n",
    "    \n",
    "    data_train, data_temp, labels_train, labels_temp = train_test_split(data, labels, test_size=(val_size + test_size), random_state=random_state)\n",
    "    data_val, data_test, labels_val, labels_test = train_test_split(data_temp, labels_temp, test_size=test_size/(test_size + val_size), random_state=random_state)\n",
    "    \n",
    "    return data_train, data_val, data_test, labels_train, labels_val, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data windows shape: torch.Size([11609, 120, 100])\n",
      "Labels windows shape: torch.Size([11609, 14])\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into windows of 100 samples each\n",
    "window_size = 100\n",
    "data_windows = data.unfold(0, window_size, window_size)\n",
    "# Create empty torch tensor to store the windows of labels\n",
    "labels_windows = torch.zeros((data_windows.shape[0], labels_data.shape[1]))\n",
    "\n",
    "cnt = 0\n",
    "# Per each window, take the mean of the labels every window_size samples\n",
    "for i in range(len(labels_data)):\n",
    "    if i + window_size <= labels_windows.shape[0]:\n",
    "        tmp_mean = torch.tensor([labels_data[i:i+window_size, j].mean() for j in range(labels_data.shape[1])])\n",
    "        labels_windows[cnt] = tmp_mean\n",
    "        cnt += 1\n",
    "        i += window_size\n",
    "\n",
    "\n",
    "# Show shape of data and labels after dividing them into windows\n",
    "print(\"\\nData windows shape:\", data_windows.shape)\n",
    "print(\"Labels windows shape:\", labels_windows.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data train shape: torch.Size([6965, 120, 100])\n",
      "Labels train shape: torch.Size([6965, 14])\n",
      "\n",
      "Data validation shape: torch.Size([1741, 120, 100])\n",
      "Labels validation shape: torch.Size([1741, 14])\n",
      "\n",
      "Data test shape: torch.Size([2903, 120, 100])\n",
      "Labels test shape: torch.Size([2903, 14])\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training, validation and test sets based on the following ratios\n",
    "train_size = 0.6\n",
    "val_size = 0.15\n",
    "test_size = 0.25\n",
    "\n",
    "# Split the data into training, validation and test sets\n",
    "data_train, data_val, data_test, labels_train, labels_val, labels_test = train_val_test_split(data_windows, labels_windows, train_size=train_size, val_size=val_size, test_size=test_size, random_state=42)\n",
    "\n",
    "# Show shape of data and labels after splitting them into training, validation and test sets\n",
    "print(\"\\nData train shape:\", data_train.shape)\n",
    "print(\"Labels train shape:\", labels_train.shape)\n",
    "print(\"\\nData validation shape:\", data_val.shape)\n",
    "print(\"Labels validation shape:\", labels_val.shape)\n",
    "print(\"\\nData test shape:\", data_test.shape)\n",
    "print(\"Labels test shape:\", labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Train Loss: 0.8217, Validation Loss: 0.8932\n",
      "Epoch [20/100], Train Loss: 0.8519, Validation Loss: 0.8932\n",
      "Epoch [30/100], Train Loss: 0.8253, Validation Loss: 0.8932\n",
      "Epoch [40/100], Train Loss: 0.8192, Validation Loss: 0.8932\n"
     ]
    }
   ],
   "source": [
    "# Create a baseline FC neural network model.\n",
    "# The model will have 10 hidden layers with 100 neurons each, and a ReLU activation function.\n",
    "# The input size is the number of features * window_size, and the output size is the number of labels.\n",
    "\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_hidden_layers):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(self.input_size, self.hidden_size))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            self.layers.append(nn.Linear(self.hidden_size, self.hidden_size))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Linear(self.hidden_size, self.output_size))\n",
    "        self.layers.append(nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "# Define the hyperparameters\n",
    "input_size = data_train.shape[1] * data_train.shape[2]\n",
    "output_size = labels_train.shape[1]\n",
    "hidden_size = 10\n",
    "num_hidden_layers = 10\n",
    "# Create the model\n",
    "model = FCNN(input_size, output_size, hidden_size, num_hidden_layers)\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function (Cross-Entropy loss) and the optimizer (Adam)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the batch size and the number of epochs for stochastic gradient descent\n",
    "batch_size = 256\n",
    "num_epochs = 1e2\n",
    "\n",
    "# Create DataLoaders for the training and validation data\n",
    "train_dataset = TensorDataset(data_train, labels_train)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(data_val, labels_val)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Lists to store loss values\n",
    "train_loss_values = []\n",
    "val_loss_values = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(int(num_epochs)):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Flatten the input data\n",
    "        data = data.view(data.size(0), -1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "    # Average training loss for the epoch\n",
    "    epoch_train_loss /= len(train_loader)\n",
    "    train_loss_values.append(epoch_train_loss)\n",
    "    \n",
    "    # Calculate validation loss\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in val_loader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Flatten the input data\n",
    "            data = data.view(data.size(0), -1)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            epoch_val_loss += loss.item()\n",
    "    \n",
    "    # Average validation loss for the epoch\n",
    "    epoch_val_loss /= len(val_loader)\n",
    "    val_loss_values.append(epoch_val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch + 1, int(num_epochs), epoch_train_loss, epoch_val_loss))\n",
    "\n",
    "# Plot the loss values\n",
    "plt.plot(range(int(num_epochs)), train_loss_values, label='Train Loss')\n",
    "plt.plot(range(int(num_epochs)), val_loss_values, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss vs Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 96.27%\n",
      "Test Accuracy: 96.18%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set - calculate the accuracy\n",
    "model.eval()\n",
    "val_correct = 0\n",
    "val_total = 0\n",
    "with torch.no_grad():\n",
    "    for data, labels in val_loader:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Flatten the input data\n",
    "        data = data.view(data.size(0), -1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        \n",
    "        # Apply threshold to get predicted labels\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        val_correct += (predicted == labels).all(dim=1).sum().item()\n",
    "        val_total += labels.size(0)\n",
    "\n",
    "val_accuracy = val_correct / val_total\n",
    "print('Validation Accuracy: {:.2f}%'.format(100 * val_accuracy))\n",
    "\n",
    "# Evaluate the model on the test set - calculate the accuracy\n",
    "test_dataset = TensorDataset(data_test, labels_test)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Flatten the input data\n",
    "        data = data.view(data.size(0), -1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        \n",
    "        # Apply threshold to get predicted labels\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        test_correct += (predicted == labels).all(dim=1).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "test_accuracy = test_correct / test_total\n",
    "print('Test Accuracy: {:.2f}%'.format(100 * test_accuracy))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
